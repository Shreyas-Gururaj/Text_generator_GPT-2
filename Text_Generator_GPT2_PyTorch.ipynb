{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7547e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers   # Transformers library from Huggingface\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e3275",
   "metadata": {},
   "source": [
    "# Load GPT-2 model and the tokenizer from the transformers library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04fae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "# Loading in the pretrained GPT-2 model itself.\n",
    "gpt_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ab242",
   "metadata": {},
   "source": [
    "## Function to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82cac515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt_text, tokenizer, model, n_seqs=1, max_length=25):\n",
    "    # n_seqs is the number of sentences to generate\n",
    "    # max_length is the maximum length of the sentence\n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    # Encoding the text using the gpt tokenizer. The return tensors are of type \"pt\" since we are using PyTorch.\n",
    "    output_sequences = model.generate(\n",
    "      input_ids=encoded_prompt,\n",
    "      max_length=max_length+len(encoded_prompt), # The model has to generate something, so add the length of the original sequence to max_length\n",
    "      temperature=0.7,   # applied to softmax to control the liklihood of high and low frequency words\n",
    "      \n",
    "      # Top-K considers only top 6 most likely words from the probability distribution while sampling (Here we          are setting it to 0 and consider Top-p(nucleus) sampling)\n",
    "      top_k=0,\n",
    "      # Top-p considers all the words from the distribution with conditional probability less than set value\n",
    "      top_p=0.9,  \n",
    "      repetition_penalty=1.2, # To ensure that we dont get repeated phrases\n",
    "      do_sample=True,\n",
    "      num_return_sequences=n_seqs   # returns a tuple of tensors containing sequence indices and the generated sequence itself.\n",
    "   ) # We feed the encoded input into the model.\n",
    "    \n",
    "    if len(output_sequences.shape) > 2:\n",
    "        output_sequences.squeeze_() # the _ indicates that the operation will be done in-place.\n",
    "    generated_sequences = []\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "        generated_sequence = generated_sequence.tolist()  # Covert the output sequence to list.\n",
    "        text = tokenizer.decode(generated_sequence)  # Decode and convert the tokens to text.\n",
    "        total_sequence = (prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True, )) :])\n",
    "        generated_sequences.append(total_sequence)\n",
    "        generated_sequences[0] = generated_sequences[0].replace('\\n','')\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f2f8f1",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Time to check our model output.\n",
    "\n",
    "generated_output = generate_text(\"electric vehicles\",\n",
    "         gpt_tokenizer,\n",
    "         gpt_model,\n",
    "         max_length=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b7fc52",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'electric vehicles (EVs) and electric buses, which are capable of running on electricity from the grid.The price for electricity is expected to fall by 2 percent this year due to a drop in coal use.\"If we take into account that coal-fired power stations will be shut down globally at an average rate of one per week by 2030, the carbon emissions from diesel-fuelled vehicles will continue falling,\" said Tim Buckley, head of fuel analysis for IHS Markit\\'s Global Fuel & Energy Outlook 2016.Meanwhile, a report released last month by International Energy Agency forecast global demand for energy would rise by 9 billion tons in 2015 and 1.2 trillion tons in 2020 as economies recover.Europe will remain the biggest market for EVs over the next three years, accounting for 30 percent of new sales, followed by China with 18 percent growth and India with 11 percent, according to IEA data.\"We\\'re seeing a big shift towards cleaner'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "generated_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098d69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}