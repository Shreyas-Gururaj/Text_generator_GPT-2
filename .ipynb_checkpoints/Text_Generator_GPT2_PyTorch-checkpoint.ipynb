{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa7547e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers   # Transformers library from Huggingface\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e3275",
   "metadata": {},
   "source": [
    "# Load GPT-2 model and the tokenizer from the transformers library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d04fae29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0609f86760da45e1a3b398019d963c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a2471386f04d3894094bfedb8a79c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1765ff760e364daaa04b8ee3f6d083fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac15b39cf6ab4dbaa6939fbc5f96d8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/764 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971ebdc5f5434195b2ce6120a9c7d059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "# Loading in the pretrained GPT-2 model itself.\n",
    "gpt_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ab242",
   "metadata": {},
   "source": [
    "## Function to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82cac515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt_text, tokenizer, model, n_seqs=1, max_length=25):\n",
    "    # n_seqs is the number of sentences to generate\n",
    "    # max_length is the maximum length of the sentence\n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    # Encoding the text using the gpt tokenizer. The return tensors are of type \"pt\" since we are using PyTorch.\n",
    "    output_sequences = model.generate(\n",
    "      input_ids=encoded_prompt,\n",
    "      max_length=max_length+len(encoded_prompt), # The model has to generate something, so add the length of the original sequence to max_length\n",
    "      temperature=1.0,\n",
    "      top_k=0,\n",
    "      top_p=0.9,\n",
    "      repetition_penalty=1.2, # To ensure that we dont get repeated phrases\n",
    "      do_sample=True,\n",
    "      num_return_sequences=n_seqs   # returns a tuple of tensors containing sequence indices and the generated sequence itself.\n",
    "   ) # We feed the encoded input into the model.\n",
    "    \n",
    "    if len(output_sequences.shape) > 2:\n",
    "        output_sequences.squeeze_() # the _ indicates that the operation will be done in-place.\n",
    "    generated_sequences = []\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "        generated_sequence = generated_sequence.tolist()  # Covert the output sequence to list.\n",
    "        text = tokenizer.decode(generated_sequence)  # Decode and convert the tokens to text.\n",
    "        total_sequence = (prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True, )) :])\n",
    "        generated_sequences.append(total_sequence)\n",
    "        generated_sequences[0] = generated_sequences[0].replace('\\n','')\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15f2f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Time to check our model output.\n",
    "\n",
    "generated_output = generate_text(\"pollution\",\n",
    "         gpt_tokenizer,\n",
    "         gpt_model,\n",
    "         max_length=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9b7fc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pollution\" or \"industrial pollution\", and then try to prove that \"something is changing\". In reality, none of this happens. It\\'s very much the opposite: namely, we are doing less destruction now than in any other historical period.But this isn\\'t a death-knell, even though it was almost instantaneous! Probably the most important natural thing to be fought against today is energy, especially electric power (electricity is produced by plants). To transform all its use into renewable energy would consume over 30 % of existing production capacity worldwide! The world resources in 2000 were only 20 % of present peak them! For every 15 GW installed, there was only 5% reduction in demand for electricity. But as speeded up, over 30 % more production capacity gets extinguished each year! Any question about rising prices? Well, we are producing more energy already than we will spend on new hydrocarbons in 2050 anyway (we can use nukes), so why should we make any effort?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098d69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
