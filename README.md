# Text_generator_GPT-2

* Uses top-p(nucleus) sampling
* Pretrained GPT-2 model, tokenizer and encoder provided by huggingface library
* Generates multiple sequences of texts of prescribed maximum length of tokens
